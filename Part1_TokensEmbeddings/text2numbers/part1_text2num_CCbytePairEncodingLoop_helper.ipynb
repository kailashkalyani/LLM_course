{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8_-VKZNDmzB"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Byte-pair encoding to a desired vocab size<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "APFQtXa0brAf"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrCWDXUdTS_N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRwQVVLT6ZNP"
   },
   "source": [
    "# Initialize the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6M7HN5nLTmaq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \" appears 7 times.\n",
      "\"a\" appears 1 times.\n",
      "\"e\" appears 5 times.\n",
      "\"g\" appears 5 times.\n",
      "\"h\" appears 4 times.\n",
      "\"i\" appears 3 times.\n",
      "\"k\" appears 2 times.\n",
      "\"l\" appears 5 times.\n",
      "\"n\" appears 1 times.\n",
      "\"o\" appears 2 times.\n",
      "\"r\" appears 2 times.\n",
      "\"s\" appears 2 times.\n",
      "\"t\" appears 1 times.\n",
      "\"u\" appears 3 times.\n",
      "\"v\" appears 2 times.\n",
      "\"y\" appears 1 times.\n"
     ]
    }
   ],
   "source": [
    "# some text with lots of repetitions\n",
    "text = 'like liker love lovely hug hugs hugging hearts'\n",
    "\n",
    "chars = list(set(text)) # the unique characters as a list\n",
    "chars.sort() # initial vocab is sorted\n",
    "\n",
    "for l in chars:\n",
    "  print(f'\"{l}\" appears {text.count(l)} times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mXFIcoLk7Q8F"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,\n",
       " {' ': 0,\n",
       "  'a': 1,\n",
       "  'e': 2,\n",
       "  'g': 3,\n",
       "  'h': 4,\n",
       "  'i': 5,\n",
       "  'k': 6,\n",
       "  'l': 7,\n",
       "  'n': 8,\n",
       "  'o': 9,\n",
       "  'r': 10,\n",
       "  's': 11,\n",
       "  't': 12,\n",
       "  'u': 13,\n",
       "  'v': 14,\n",
       "  'y': 15})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a vocabulary\n",
    "vocab = {c:i  for i,c in enumerate(chars)  }\n",
    "len(vocab), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "k70QKgFkBCr2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like liker love lovely hug hugs hugging hearts\n",
      "['l', 'i', 'k', 'e', ' ', 'l', 'i', 'k', 'e', 'r', ' ', 'l', 'o', 'v', 'e', ' ', 'l', 'o', 'v', 'e', 'l', 'y', ' ', 'h', 'u', 'g', ' ', 'h', 'u', 'g', 's', ' ', 'h', 'u', 'g', 'g', 'i', 'n', 'g', ' ', 'h', 'e', 'a', 'r', 't', 's']\n"
     ]
    }
   ],
   "source": [
    "# the text needs to be a list, not a string\n",
    "# each element in the list is a token\n",
    "origtext = list(text)\n",
    "print(text)\n",
    "print(origtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ky9njLdZUgVR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Smkdx0MUgRx"
   },
   "source": [
    "# Now using functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "DrX7ThhHcqk3"
   },
   "outputs": [],
   "source": [
    "# > -----------------------------------\n",
    "def get_pair_stats(text2pair):\n",
    "  token_pairs = dict()\n",
    "\n",
    "  # loop over tokens\n",
    "  for i in range(len(text2pair)-1):\n",
    "\n",
    "    # create a pair\n",
    "    pair = text2pair[i]+text2pair[i+1]# this letter and the next\n",
    "\n",
    "    # increase pair frequencies\n",
    "    if pair in token_pairs: # exists already, increment by 1\n",
    "      token_pairs[pair]+=1\n",
    "    else: # initialize it\n",
    "      token_pairs[pair]=1\n",
    "\n",
    "  return token_pairs\n",
    "# ----------------------------------- <\n",
    "\n",
    "\n",
    "\n",
    "# > -----------------------------------\n",
    "def update_vocab(token_pairs,vocab):\n",
    "\n",
    "  # find the most frequent pair\n",
    "  idx = np.argmax(list(token_pairs.values()))# index of maximum of token_pairs\n",
    "  #print(idx, list(token_pairs.values()))\n",
    "  newtok =  list(token_pairs.keys())[idx]\n",
    "  #print (newtok)\n",
    "\n",
    "  # update the vocab\n",
    "  vocab[newtok] = len(vocab)# a new token index\n",
    "  return vocab,newtok\n",
    "# ----------------------------------- <\n",
    "\n",
    "\n",
    "\n",
    "# > -----------------------------------\n",
    "def generate_new_token_seq(prevtext,newtoken):\n",
    "\n",
    "  # initialize a new text list\n",
    "  newtext = []\n",
    "\n",
    "  # loop through the list\n",
    "  i = 0\n",
    "  while i<(len(prevtext)-1):\n",
    "\n",
    "    # test whether the pair of this and the following elements match the newly-created pair\n",
    "    if prevtext[i]+prevtext[i+1]==newtoken: # this and next token equals new token\n",
    "      newtext.append(newtoken)\n",
    "      i+=2\n",
    "      # skip the next character\n",
    "\n",
    "    # not a pair\n",
    "    else:\n",
    "      newtext.append(prevtext[i])\n",
    "      i+=1\n",
    "      # move to the next character\n",
    "\n",
    "  # the following code was missing in the video\n",
    "  if i < len(prevtext):\n",
    "    newtext.append(prevtext[i])\n",
    "\n",
    "  return newtext\n",
    "# ----------------------------------- <"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaBv1eDocqhv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQUP3Mu7X501"
   },
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2CA-ybfLLgil"
   },
   "outputs": [],
   "source": [
    "# re-initialize the vocab (again...)\n",
    "vocab = {c:i  for i,c in enumerate(chars)  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "qxIZ6KCcX5x0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab has 17 tokens after adding \" h\"\n",
      "Vocab has 18 tokens after adding \" l\"\n",
      "Vocab has 19 tokens after adding \" hu\"\n",
      "Vocab has 20 tokens after adding \" hug\"\n",
      "Vocab has 21 tokens after adding \"ik\"\n",
      "Vocab has 22 tokens after adding \"ike\"\n",
      "Vocab has 23 tokens after adding \" lo\"\n",
      "Vocab has 24 tokens after adding \" lov\"\n",
      "Vocab has 25 tokens after adding \" love\"\n"
     ]
    }
   ],
   "source": [
    "# how many tokens in the vocab?\n",
    "vocab_size = 25\n",
    "\n",
    "# make a copy of the original text\n",
    "updated_text = origtext.copy()\n",
    "\n",
    "while len(vocab)<vocab_size:\n",
    "\n",
    "    # get pair statistics\n",
    "    pairs = get_pair_stats(updated_text)\n",
    "\n",
    "    #print(pairs)\n",
    "\n",
    "    # update the dictionary\n",
    "    vocab,newtoken =update_vocab(pairs, vocab)\n",
    "\n",
    "    # get a new list of tokens\n",
    "\n",
    "    print(f'Vocab has {len(vocab)} tokens after adding \"{newtoken}\"')\n",
    "    updated_text = generate_new_token_seq(updated_text,newtoken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Q3krn5y2-Cnp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'ike', ' l', 'ike', 'r', ' love', ' love', 'l', 'y', ' hug', ' hug', 's', ' hug', 'g', 'i', 'n', 'g', ' h', 'e', 'a', 'r', 't', 's']\n"
     ]
    }
   ],
   "source": [
    "# final tokenized text\n",
    "print(updated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0avzsroUNvfC"
   },
   "outputs": [],
   "source": [
    "# the final vocab\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zwgt0JTNw0y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
