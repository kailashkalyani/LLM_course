{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8_-VKZNDmzB"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>CodeChallenge HELPER: Byte-pair encoding to a desired vocab size<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "APFQtXa0brAf"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrCWDXUdTS_N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRwQVVLT6ZNP"
   },
   "source": [
    "# Initialize the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6M7HN5nLTmaq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \" appears 7 times.\n",
      "\"a\" appears 1 times.\n",
      "\"e\" appears 5 times.\n",
      "\"g\" appears 5 times.\n",
      "\"h\" appears 4 times.\n",
      "\"i\" appears 3 times.\n",
      "\"k\" appears 2 times.\n",
      "\"l\" appears 5 times.\n",
      "\"n\" appears 1 times.\n",
      "\"o\" appears 2 times.\n",
      "\"r\" appears 2 times.\n",
      "\"s\" appears 2 times.\n",
      "\"t\" appears 1 times.\n",
      "\"u\" appears 3 times.\n",
      "\"v\" appears 2 times.\n",
      "\"y\" appears 1 times.\n"
     ]
    }
   ],
   "source": [
    "# some text with lots of repetitions\n",
    "text = 'like liker love lovely hug hugs hugging hearts'\n",
    "\n",
    "chars = list(set(text)) # the unique characters as a list\n",
    "chars.sort() # initial vocab is sorted\n",
    "\n",
    "for l in chars:\n",
    "  print(f'\"{l}\" appears {text.count(l)} times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mXFIcoLk7Q8F"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,\n",
       " {' ': 0,\n",
       "  'a': 1,\n",
       "  'e': 2,\n",
       "  'g': 3,\n",
       "  'h': 4,\n",
       "  'i': 5,\n",
       "  'k': 6,\n",
       "  'l': 7,\n",
       "  'n': 8,\n",
       "  'o': 9,\n",
       "  'r': 10,\n",
       "  's': 11,\n",
       "  't': 12,\n",
       "  'u': 13,\n",
       "  'v': 14,\n",
       "  'y': 15})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a vocabulary\n",
    "vocab = {c:i  for i,c in enumerate(chars)  }\n",
    "len(vocab), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "k70QKgFkBCr2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like liker love lovely hug hugs hugging hearts\n",
      "['l', 'i', 'k', 'e', ' ', 'l', 'i', 'k', 'e', 'r', ' ', 'l', 'o', 'v', 'e', ' ', 'l', 'o', 'v', 'e', 'l', 'y', ' ', 'h', 'u', 'g', ' ', 'h', 'u', 'g', 's', ' ', 'h', 'u', 'g', 'g', 'i', 'n', 'g', ' ', 'h', 'e', 'a', 'r', 't', 's']\n"
     ]
    }
   ],
   "source": [
    "# the text needs to be a list, not a string\n",
    "# each element in the list is a token\n",
    "origtext = list(text)\n",
    "print(text)\n",
    "print(origtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ky9njLdZUgVR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Smkdx0MUgRx"
   },
   "source": [
    "# Now using functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrX7ThhHcqk3"
   },
   "outputs": [],
   "source": [
    "# > -----------------------------------\n",
    "def get_pair_stats(text2pair):\n",
    "  token_pairs = dict()\n",
    "\n",
    "  # loop over tokens\n",
    "  for i in range(len(text2pair)-1):\n",
    "\n",
    "    # create a pair\n",
    "    pair = text2pair[i]+text2pair[i+1]# this letter and the next\n",
    "\n",
    "    # increase pair frequencies\n",
    "    if pair in token_pairs: # exists already, increment by 1\n",
    "      token_pairs[pair]+=1\n",
    "    else: # initialize it\n",
    "      token_pairs[pair]=1\n",
    "\n",
    "  return token_pairs\n",
    "# ----------------------------------- <\n",
    "\n",
    "\n",
    "\n",
    "# > -----------------------------------\n",
    "def update_vocab(token_pairs,vocab):\n",
    "\n",
    "  # find the most frequent pair\n",
    "  idx = np.argmax(list(token_pairs.values()))# index of maximum of token_pairs\n",
    "  print(idx, list(token_pairs.values()))\n",
    "  newtok =  list(token_pairs.keys())[idx]\n",
    "  print (newtok)\n",
    "\n",
    "  # update the vocab\n",
    "  vocab[newtok] = len(vocab)# a new token index\n",
    "  return vocab,newtok\n",
    "# ----------------------------------- <\n",
    "\n",
    "\n",
    "\n",
    "# > -----------------------------------\n",
    "def generate_new_token_seq(prevtext,newtoken):\n",
    "\n",
    "  # initialize a new text list\n",
    "  newtext = []\n",
    "\n",
    "  # loop through the list\n",
    "  i = 0\n",
    "  while i<(len(prevtext)-1):\n",
    "\n",
    "    # test whether the pair of this and the following elements match the newly-created pair\n",
    "    if prevtext[i]+prevtext[i+1]==newtoken: # this and next token equals new token\n",
    "      newtext.append(newtoken)\n",
    "      i+=2\n",
    "      # skip the next character\n",
    "\n",
    "    # not a pair\n",
    "    else:\n",
    "      newtext.append(prevtext[i])\n",
    "      i+=1\n",
    "      # move to the next character\n",
    "\n",
    "  # the following code was missing in the video\n",
    "  if i < len(prevtext):\n",
    "    newtext.append(prevtext[i])\n",
    "\n",
    "  return newtext\n",
    "# ----------------------------------- <"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaBv1eDocqhv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQUP3Mu7X501"
   },
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2CA-ybfLLgil"
   },
   "outputs": [],
   "source": [
    "# re-initialize the vocab (again...)\n",
    "vocab = {c:i  for i,c in enumerate(chars)  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qxIZ6KCcX5x0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'li': 2, 'ik': 2, 'ke': 2, 'e ': 2, ' l': 3, 'er': 1, 'r ': 1, 'lo': 2, 'ov': 2, 've': 2, 'el': 1, 'ly': 1, 'y ': 1, ' h': 4, 'hu': 3, 'ug': 3, 'g ': 2, 'gs': 1, 's ': 1, 'gg': 1, 'gi': 1, 'in': 1, 'ng': 1, 'he': 1, 'ea': 1, 'ar': 1, 'rt': 1, 'ts': 1}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(pairs)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# update the dictionary\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m vocab,newtoken =\u001b[43mupdate_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# get a new list of tokens\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mVocab has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vocab)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tokens after adding \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnewtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mupdate_vocab\u001b[39m\u001b[34m(token_pairs, vocab)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_vocab\u001b[39m(token_pairs,vocab):\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m   \u001b[38;5;66;03m# find the most frequent pair\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m   idx = np.argmax(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken_pairs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m())\u001b[38;5;66;03m# index of maximum of token_pairs\u001b[39;00m\n\u001b[32m     27\u001b[39m   \u001b[38;5;28mprint\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m(token_pairs.values()))\n\u001b[32m     28\u001b[39m   newtok =  \u001b[38;5;28mlist\u001b[39m(token_pairs.keys())[idx]\n",
      "\u001b[31mTypeError\u001b[39m: 'builtin_function_or_method' object is not iterable"
     ]
    }
   ],
   "source": [
    "# how many tokens in the vocab?\n",
    "vocab_size = 25\n",
    "\n",
    "# make a copy of the original text\n",
    "updated_text = origtext.copy()\n",
    "\n",
    "#while len(vocab)<vocab_size:\n",
    "\n",
    "# get pair statistics\n",
    "pairs = get_pair_stats(updated_text)\n",
    "\n",
    "print(pairs)\n",
    "\n",
    "# update the dictionary\n",
    "vocab,newtoken =update_vocab(pairs, vocab)\n",
    "\n",
    "# get a new list of tokens\n",
    "\n",
    "print(f'Vocab has {len(vocab)} tokens after adding \"{newtoken}\"')\n",
    "updated_text = generate_new_token_seq(updated_text,newtoken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3krn5y2-Cnp"
   },
   "outputs": [],
   "source": [
    "# final tokenized text\n",
    "print(updated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0avzsroUNvfC"
   },
   "outputs": [],
   "source": [
    "# the final vocab\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zwgt0JTNw0y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
