{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnNdFahh7-8z"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>Exploring ChatGPT4's tokenizer<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1UFBQ9o7-6J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C8JzcByJ_dVt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# matplotlib defaults\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O-rylt4G8MWp"
   },
   "outputs": [],
   "source": [
    "# need to install the tiktoken library to get OpenAI's tokenizer\n",
    "# note: it's tik-token, not tiktok-en :P\n",
    "#!pip install tiktoken\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vll_zH5t_gHC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f34QAbA-_gD-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_core_bpe',\n",
       " '_encode_bytes',\n",
       " '_encode_only_native_bpe',\n",
       " '_encode_single_piece',\n",
       " '_mergeable_ranks',\n",
       " '_pat_str',\n",
       " '_special_tokens',\n",
       " 'decode',\n",
       " 'decode_batch',\n",
       " 'decode_bytes',\n",
       " 'decode_bytes_batch',\n",
       " 'decode_single_token_bytes',\n",
       " 'decode_tokens_bytes',\n",
       " 'decode_with_offsets',\n",
       " 'encode',\n",
       " 'encode_batch',\n",
       " 'encode_ordinary',\n",
       " 'encode_ordinary_batch',\n",
       " 'encode_single_token',\n",
       " 'encode_to_numpy',\n",
       " 'encode_with_unstable',\n",
       " 'eot_token',\n",
       " 'is_special_token',\n",
       " 'max_token_value',\n",
       " 'n_vocab',\n",
       " 'name',\n",
       " 'special_tokens_set',\n",
       " 'token_byte_values']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT-4's tokenizer\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "D-ehv3aujvgh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mType:\u001b[39m           Encoding\n",
      "\u001b[31mString form:\u001b[39m    <Encoding 'cl100k_base'>\n",
      "\u001b[31mFile:\u001b[39m           ~/Code/LLM_course/Part1_TokensEmbeddings/text2numbers/.venv/lib/python3.12/site-packages/tiktoken/core.py\n",
      "\u001b[31mSource:\u001b[39m        \n",
      "\u001b[38;5;28;01mclass\u001b[39;00m Encoding:\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __init__(\n",
      "        self,\n",
      "        name: str,\n",
      "        *,\n",
      "        pat_str: str,\n",
      "        mergeable_ranks: dict[bytes, int],\n",
      "        special_tokens: dict[str, int],\n",
      "        explicit_n_vocab: int | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ):\n",
      "        \u001b[33m\"\"\"Creates an Encoding object.\u001b[39m\n",
      "\n",
      "\u001b[33m        See openai_public.py for examples of how to construct an Encoding object.\u001b[39m\n",
      "\n",
      "\u001b[33m        Args:\u001b[39m\n",
      "\u001b[33m            name: The name of the encoding. It should be clear from the name of the encoding\u001b[39m\n",
      "\u001b[33m                what behaviour to expect, in particular, encodings with different special tokens\u001b[39m\n",
      "\u001b[33m                should have different names.\u001b[39m\n",
      "\u001b[33m            pat_str: A regex pattern string that is used to split the input text.\u001b[39m\n",
      "\u001b[33m            mergeable_ranks: A dictionary mapping mergeable token bytes to their ranks. The ranks\u001b[39m\n",
      "\u001b[33m                must correspond to merge priority.\u001b[39m\n",
      "\u001b[33m            special_tokens: A dictionary mapping special token strings to their token values.\u001b[39m\n",
      "\u001b[33m            explicit_n_vocab: The number of tokens in the vocabulary. If provided, it is checked\u001b[39m\n",
      "\u001b[33m                that the number of mergeable tokens and special tokens is equal to this number.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        self.name = name\n",
      "\n",
      "        self._pat_str = pat_str\n",
      "        self._mergeable_ranks = mergeable_ranks\n",
      "        self._special_tokens = special_tokens\n",
      "\n",
      "        self.max_token_value = max(\n",
      "            max(mergeable_ranks.values()), max(special_tokens.values(), default=\u001b[32m0\u001b[39m)\n",
      "        )\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m explicit_n_vocab:\n",
      "            \u001b[38;5;28;01massert\u001b[39;00m len(mergeable_ranks) + len(special_tokens) == explicit_n_vocab\n",
      "            \u001b[38;5;28;01massert\u001b[39;00m self.max_token_value == explicit_n_vocab - \u001b[32m1\u001b[39m\n",
      "\n",
      "        self._core_bpe = _tiktoken.CoreBPE(mergeable_ranks, special_tokens, pat_str)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __repr__(self) -> str:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m f\"<Encoding {self.name!r}>\"\n",
      "\n",
      "    \u001b[38;5;66;03m# ====================\u001b[39;00m\n",
      "    \u001b[38;5;66;03m# Encoding\u001b[39;00m\n",
      "    \u001b[38;5;66;03m# ====================\u001b[39;00m\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m encode_ordinary(self, text: str) -> list[int]:\n",
      "        \u001b[33m\"\"\"Encodes a string into tokens, ignoring special tokens.\u001b[39m\n",
      "\n",
      "\u001b[33m        This is equivalent to `encode(text, disallowed_special=())` (but slightly faster).\u001b[39m\n",
      "\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        >>> enc.encode_ordinary(\"hello world\")\u001b[39m\n",
      "\u001b[33m        [31373, 995]\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._core_bpe.encode_ordinary(text)\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m UnicodeEncodeError:\n",
      "            \u001b[38;5;66;03m# See comment in encode\u001b[39;00m\n",
      "            text = text.encode(\u001b[33m\"utf-16\"\u001b[39m, \u001b[33m\"surrogatepass\"\u001b[39m).decode(\u001b[33m\"utf-16\"\u001b[39m, \u001b[33m\"replace\"\u001b[39m)\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._core_bpe.encode_ordinary(text)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m encode(\n",
      "        self,\n",
      "        text: str,\n",
      "        *,\n",
      "        allowed_special: Literal[\u001b[33m\"all\"\u001b[39m] | AbstractSet[str] = set(),  \u001b[38;5;66;03m# noqa: B006\u001b[39;00m\n",
      "        disallowed_special: Literal[\u001b[33m\"all\"\u001b[39m] | Collection[str] = \u001b[33m\"all\"\u001b[39m,\n",
      "    ) -> list[int]:\n",
      "        \u001b[33m\"\"\"Encodes a string into tokens.\u001b[39m\n",
      "\n",
      "\u001b[33m        Special tokens are artificial tokens used to unlock capabilities from a model,\u001b[39m\n",
      "\u001b[33m        such as fill-in-the-middle. So we want to be careful about accidentally encoding special\u001b[39m\n",
      "\u001b[33m        tokens, since they can be used to trick a model into doing something we don't want it to do.\u001b[39m\n",
      "\n",
      "\u001b[33m        Hence, by default, encode will raise an error if it encounters text that corresponds\u001b[39m\n",
      "\u001b[33m        to a special token. This can be controlled on a per-token level using the `allowed_special`\u001b[39m\n",
      "\u001b[33m        and `disallowed_special` parameters. In particular:\u001b[39m\n",
      "\u001b[33m        - Setting `disallowed_special` to () will prevent this function from raising errors and\u001b[39m\n",
      "\u001b[33m          cause all text corresponding to special tokens to be encoded as natural text.\u001b[39m\n",
      "\u001b[33m        - Setting `allowed_special` to \"all\" will cause this function to treat all text\u001b[39m\n",
      "\u001b[33m          corresponding to special tokens to be encoded as special tokens.\u001b[39m\n",
      "\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        >>> enc.encode(\"hello world\")\u001b[39m\n",
      "\u001b[33m        [31373, 995]\u001b[39m\n",
      "\u001b[33m        >>> enc.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})\u001b[39m\n",
      "\u001b[33m        [50256]\u001b[39m\n",
      "\u001b[33m        >>> enc.encode(\"<|endoftext|>\", allowed_special=\"all\")\u001b[39m\n",
      "\u001b[33m        [50256]\u001b[39m\n",
      "\u001b[33m        >>> enc.encode(\"<|endoftext|>\")\u001b[39m\n",
      "\u001b[33m        # Raises ValueError\u001b[39m\n",
      "\u001b[33m        >>> enc.encode(\"<|endoftext|>\", disallowed_special=())\u001b[39m\n",
      "\u001b[33m        [27, 91, 437, 1659, 5239, 91, 29]\u001b[39m\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m allowed_special == \u001b[33m\"all\"\u001b[39m:\n",
      "            allowed_special = self.special_tokens_set\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m disallowed_special == \u001b[33m\"all\"\u001b[39m:\n",
      "            disallowed_special = self.special_tokens_set - allowed_special\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m disallowed_special:\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(disallowed_special, frozenset):\n",
      "                disallowed_special = frozenset(disallowed_special)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m match := _special_token_regex(disallowed_special).search(text):\n",
      "                raise_disallowed_special_token(match.group())\n",
      "\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._core_bpe.encode(text, allowed_special)\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m UnicodeEncodeError:\n",
      "            \u001b[38;5;66;03m# BPE operates on bytes, but the regex operates on unicode. If we pass a str that is\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# invalid UTF-8 to Rust, it will rightfully complain. Here we do a quick and dirty\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# fixup for any surrogate pairs that may have sneaked their way into the text.\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# Technically, this introduces a place where encode + decode doesn't roundtrip a Python\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# string, but given that this is input we want to support, maybe that's okay.\u001b[39;00m\n",
      "            \u001b[38;5;66;03m# Also we use errors=\"replace\" to handle weird things like lone surrogates.\u001b[39;00m\n",
      "            text = text.encode(\u001b[33m\"utf-16\"\u001b[39m, \u001b[33m\"surrogatepass\"\u001b[39m).decode(\u001b[33m\"utf-16\"\u001b[39m, \u001b[33m\"replace\"\u001b[39m)\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self._core_bpe.encode(text, allowed_special)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m encode_to_numpy(\n",
      "        self,\n",
      "        text: str,\n",
      "        *,\n",
      "        allowed_special: Literal[\u001b[33m\"all\"\u001b[39m] | AbstractSet[str] = set(),  \u001b[38;5;66;03m# noqa: B006\u001b[39;00m\n",
      "        disallowed_special: Literal[\u001b[33m\"all\"\u001b[39m] | Collection[str] = \u001b[33m\"all\"\u001b[39m,\n",
      "    ) -> npt.NDArray[np.uint32]:\n",
      "        \u001b[33m\"\"\"Encodes a string into tokens, returning a numpy array.\u001b[39m\n",
      "\n",
      "\u001b[33m        Avoids the overhead of copying the token buffer into a Python list.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m allowed_special == \u001b[33m\"all\"\u001b[39m:\n",
      "            allowed_special = self.special_tokens_set\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m disallowed_special == \u001b[33m\"all\"\u001b[39m:\n",
      "            disallowed_special = self.special_tokens_set - allowed_special\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m disallowed_special:\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(disallowed_special, frozenset):\n",
      "                disallowed_special = frozenset(disallowed_special)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m match := _special_token_regex(disallowed_special).search(text):\n",
      "                raise_disallowed_special_token(match.group())\n",
      "\n",
      "        \u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np\n",
      "\n",
      "        buffer = self._core_bpe.encode_to_tiktoken_buffer(text, allowed_special)\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m np.frombuffer(buffer, dtype=np.uint32)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m encode_ordinary_batch(self, text: list[str], *, num_threads: int = \u001b[32m8\u001b[39m) -> list[list[int]]:\n",
      "        \u001b[33m\"\"\"Encodes a list of strings into tokens, in parallel, ignoring special tokens.\u001b[39m\n",
      "\n",
      "\u001b[33m        This is equivalent to `encode_batch(text, disallowed_special=())` (but slightly faster).\u001b[39m\n",
      "\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        >>> enc.encode_ordinary_batch([\"hello world\", \"goodbye world\"])\u001b[39m\n",
      "\u001b[33m        [[31373, 995], [11274, 16390, 995]]\u001b[39m\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        encoder = functools.partial(self.encode_ordinary)\n",
      "        \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(num_threads) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m list(e.map(encoder, text))\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m encode_batch(\n",
      "        self,\n",
      "        text: list[str],\n",
      "        *,\n",
      "        num_threads: int = \u001b[32m8\u001b[39m,\n",
      "        allowed_special: Literal[\u001b[33m\"all\"\u001b[39m] | AbstractSet[str] = set(),  \u001b[38;5;66;03m# noqa: B006\u001b[39;00m\n",
      "        disallowed_special: Literal[\u001b[33m\"all\"\u001b[39m] | Collection[str] = \u001b[33m\"all\"\u001b[39m,\n",
      "    ) -> list[list[int]]:\n",
      "        \u001b[33m\"\"\"Encodes a list of strings into tokens, in parallel.\u001b[39m\n",
      "\n",
      "\u001b[33m        See `encode` for more details on `allowed_special` and `disallowed_special`.\u001b[39m\n",
      "\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        >>> enc.encode_batch([\"hello world\", \"goodbye world\"])\u001b[39m\n",
      "\u001b[33m        [[31373, 995], [11274, 16390, 995]]\u001b[39m\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m allowed_special == \u001b[33m\"all\"\u001b[39m:\n",
      "            allowed_special = self.special_tokens_set\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m disallowed_special == \u001b[33m\"all\"\u001b[39m:\n",
      "            disallowed_special = self.special_tokens_set - allowed_special\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(disallowed_special, frozenset):\n",
      "            disallowed_special = frozenset(disallowed_special)\n",
      "\n",
      "        encoder = functools.partial(\n",
      "            self.encode, allowed_special=allowed_special, disallowed_special=disallowed_special\n",
      "        )\n",
      "        \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(num_threads) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m list(e.map(encoder, text))\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m encode_with_unstable(\n",
      "        self,\n",
      "        text: str,\n",
      "        *,\n",
      "        allowed_special: Literal[\u001b[33m\"all\"\u001b[39m] | AbstractSet[str] = set(),  \u001b[38;5;66;03m# noqa: B006\u001b[39;00m\n",
      "        disallowed_special: Literal[\u001b[33m\"all\"\u001b[39m] | Collection[str] = \u001b[33m\"all\"\u001b[39m,\n",
      "    ) -> tuple[list[int], list[list[int]]]:\n",
      "        \u001b[33m\"\"\"Encodes a string into stable tokens and possible completion sequences.\u001b[39m\n",
      "\n",
      "\u001b[33m        Note that the stable tokens will only represent a substring of `text`.\u001b[39m\n",
      "\n",
      "\u001b[33m        See `encode` for more details on `allowed_special` and `disallowed_special`.\u001b[39m\n",
      "\n",
      "\u001b[33m        This API should itself be considered unstable.\u001b[39m\n",
      "\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        >>> enc.encode_with_unstable(\"hello fanta\")\u001b[39m\n",
      "\u001b[33m        ([31373], [(277, 4910), (5113, 265), ..., (8842,)])\u001b[39m\n",
      "\n",
      "\u001b[33m        >>> text = \"...\"\u001b[39m\n",
      "\u001b[33m        >>> stable_tokens, completions = enc.encode_with_unstable(text)\u001b[39m\n",
      "\u001b[33m        >>> assert text.encode().startswith(enc.decode_bytes(stable_tokens))\u001b[39m\n",
      "\u001b[33m        >>> assert all(enc.decode_bytes(stable_tokens + seq).startswith(text.encode()) for seq in completions)\u001b[39m\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m allowed_special == \u001b[33m\"all\"\u001b[39m:\n",
      "            allowed_special = self.special_tokens_set\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m disallowed_special == \u001b[33m\"all\"\u001b[39m:\n",
      "            disallowed_special = self.special_tokens_set - allowed_special\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m disallowed_special:\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(disallowed_special, frozenset):\n",
      "                disallowed_special = frozenset(disallowed_special)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m match := _special_token_regex(disallowed_special).search(text):\n",
      "                raise_disallowed_special_token(match.group())\n",
      "\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._core_bpe.encode_with_unstable(text, allowed_special)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m encode_single_token(self, text_or_bytes: str | bytes) -> int:\n",
      "        \u001b[33m\"\"\"Encodes text corresponding to a single token to its token value.\u001b[39m\n",
      "\n",
      "\u001b[33m        NOTE: this will encode all special tokens.\u001b[39m\n",
      "\n",
      "\u001b[33m        Raises `KeyError` if the token is not in the vocabulary.\u001b[39m\n",
      "\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        >>> enc.encode_single_token(\"hello\")\u001b[39m\n",
      "\u001b[33m        31373\u001b[39m\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m isinstance(text_or_bytes, str):\n",
      "            text_or_bytes = text_or_bytes.encode(\u001b[33m\"utf-8\"\u001b[39m)\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._core_bpe.encode_single_token(text_or_bytes)\n",
      "\n",
      "    \u001b[38;5;66;03m# ====================\u001b[39;00m\n",
      "    \u001b[38;5;66;03m# Decoding\u001b[39;00m\n",
      "    \u001b[38;5;66;03m# ====================\u001b[39;00m\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m decode_bytes(self, tokens: Sequence[int]) -> bytes:\n",
      "        \u001b[33m\"\"\"Decodes a list of tokens into bytes.\u001b[39m\n",
      "\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        >>> enc.decode_bytes([31373, 995])\u001b[39m\n",
      "\u001b[33m        b'hello world'\u001b[39m\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._core_bpe.decode_bytes(tokens)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m decode(self, tokens: Sequence[int], errors: str = \u001b[33m\"replace\"\u001b[39m) -> str:\n",
      "        \u001b[33m\"\"\"Decodes a list of tokens into a string.\u001b[39m\n",
      "\n",
      "\u001b[33m        WARNING: the default behaviour of this function is lossy, since decoded bytes are not\u001b[39m\n",
      "\u001b[33m        guaranteed to be valid UTF-8. You can control this behaviour using the `errors` parameter,\u001b[39m\n",
      "\u001b[33m        for instance, setting `errors=strict`.\u001b[39m\n",
      "\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        >>> enc.decode([31373, 995])\u001b[39m\n",
      "\u001b[33m        'hello world'\u001b[39m\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._core_bpe.decode_bytes(tokens).decode(\u001b[33m\"utf-8\"\u001b[39m, errors=errors)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m decode_single_token_bytes(self, token: int) -> bytes:\n",
      "        \u001b[33m\"\"\"Decodes a token into bytes.\u001b[39m\n",
      "\n",
      "\u001b[33m        NOTE: this will decode all special tokens.\u001b[39m\n",
      "\n",
      "\u001b[33m        Raises `KeyError` if the token is not in the vocabulary.\u001b[39m\n",
      "\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        >>> enc.decode_single_token_bytes(31373)\u001b[39m\n",
      "\u001b[33m        b'hello'\u001b[39m\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._core_bpe.decode_single_token_bytes(token)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m decode_tokens_bytes(self, tokens: Sequence[int]) -> list[bytes]:\n",
      "        \u001b[33m\"\"\"Decodes a list of tokens into a list of bytes.\u001b[39m\n",
      "\n",
      "\u001b[33m        Useful for visualising tokenisation.\u001b[39m\n",
      "\u001b[33m        >>> enc.decode_tokens_bytes([31373, 995])\u001b[39m\n",
      "\u001b[33m        [b'hello', b' world']\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m [self.decode_single_token_bytes(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;28;01min\u001b[39;00m tokens]\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m decode_with_offsets(self, tokens: Sequence[int]) -> tuple[str, list[int]]:\n",
      "        \u001b[33m\"\"\"Decodes a list of tokens into a string and a list of offsets.\u001b[39m\n",
      "\n",
      "\u001b[33m        Each offset is the index into text corresponding to the start of each token.\u001b[39m\n",
      "\u001b[33m        If UTF-8 character boundaries do not line up with token boundaries, the offset is the index\u001b[39m\n",
      "\u001b[33m        of the first character that contains bytes from the token.\u001b[39m\n",
      "\n",
      "\u001b[33m        This will currently raise if given tokens that decode to invalid UTF-8; this behaviour may\u001b[39m\n",
      "\u001b[33m        change in the future to be more permissive.\u001b[39m\n",
      "\n",
      "\u001b[33m        >>> enc.decode_with_offsets([31373, 995])\u001b[39m\n",
      "\u001b[33m        ('hello world', [0, 5])\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        token_bytes = self.decode_tokens_bytes(tokens)\n",
      "\n",
      "        text_len = \u001b[32m0\u001b[39m\n",
      "        offsets = []\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;28;01min\u001b[39;00m token_bytes:\n",
      "            offsets.append(max(\u001b[32m0\u001b[39m, text_len - (\u001b[32m0x80\u001b[39m <= token[\u001b[32m0\u001b[39m] < \u001b[32m0xC0\u001b[39m)))\n",
      "            text_len += sum(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;28;01min\u001b[39;00m token \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[32m0x80\u001b[39m <= c < \u001b[32m0xC0\u001b[39m)\n",
      "\n",
      "        \u001b[38;5;66;03m# TODO: assess correctness for errors=\"ignore\" and errors=\"replace\"\u001b[39;00m\n",
      "        text = \u001b[33mb\"\"\u001b[39m.join(token_bytes).decode(\u001b[33m\"utf-8\"\u001b[39m, errors=\u001b[33m\"strict\"\u001b[39m)\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m text, offsets\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m decode_batch(\n",
      "        self, batch: Sequence[Sequence[int]], *, errors: str = \u001b[33m\"replace\"\u001b[39m, num_threads: int = \u001b[32m8\u001b[39m\n",
      "    ) -> list[str]:\n",
      "        \u001b[33m\"\"\"Decodes a batch (list of lists of tokens) into a list of strings.\"\"\"\u001b[39m\n",
      "        decoder = functools.partial(self.decode, errors=errors)\n",
      "        \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(num_threads) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m list(e.map(decoder, batch))\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m decode_bytes_batch(\n",
      "        self, batch: Sequence[Sequence[int]], *, num_threads: int = \u001b[32m8\u001b[39m\n",
      "    ) -> list[bytes]:\n",
      "        \u001b[33m\"\"\"Decodes a batch (list of lists of tokens) into a list of bytes.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(num_threads) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m list(e.map(self.decode_bytes, batch))\n",
      "\n",
      "    \u001b[38;5;66;03m# ====================\u001b[39;00m\n",
      "    \u001b[38;5;66;03m# Miscellaneous\u001b[39;00m\n",
      "    \u001b[38;5;66;03m# ====================\u001b[39;00m\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m token_byte_values(self) -> list[bytes]:\n",
      "        \u001b[33m\"\"\"Returns the list of all token byte values.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._core_bpe.token_byte_values()\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m eot_token(self) -> int:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._special_tokens[\u001b[33m\"<|endoftext|>\"\u001b[39m]\n",
      "\n",
      "    @functools.cached_property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m special_tokens_set(self) -> set[str]:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m set(self._special_tokens.keys())\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m is_special_token(self, token: int) -> bool:\n",
      "        \u001b[38;5;28;01massert\u001b[39;00m isinstance(token, int)\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m token \u001b[38;5;28;01min\u001b[39;00m self._special_token_values\n",
      "\n",
      "    @property\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m n_vocab(self) -> int:\n",
      "        \u001b[33m\"\"\"For backwards compatibility. Prefer to use `enc.max_token_value + 1`.\"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self.max_token_value + \u001b[32m1\u001b[39m\n",
      "\n",
      "    \u001b[38;5;66;03m# ====================\u001b[39;00m\n",
      "    \u001b[38;5;66;03m# Private\u001b[39;00m\n",
      "    \u001b[38;5;66;03m# ====================\u001b[39;00m\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _encode_single_piece(self, text_or_bytes: str | bytes) -> list[int]:\n",
      "        \u001b[33m\"\"\"Encodes text corresponding to bytes without a regex split.\u001b[39m\n",
      "\n",
      "\u001b[33m        NOTE: this will not encode any special tokens.\u001b[39m\n",
      "\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        >>> enc.encode_single_piece(\"helloqqqq\")\u001b[39m\n",
      "\u001b[33m        [31373, 38227, 38227]\u001b[39m\n",
      "\u001b[33m        ```\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m isinstance(text_or_bytes, str):\n",
      "            text_or_bytes = text_or_bytes.encode(\u001b[33m\"utf-8\"\u001b[39m)\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._core_bpe.encode_single_piece(text_or_bytes)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _encode_only_native_bpe(self, text: str) -> list[int]:\n",
      "        \u001b[33m\"\"\"Encodes a string into tokens, but do regex splitting in Python.\"\"\"\u001b[39m\n",
      "        _unused_pat = regex.compile(self._pat_str)\n",
      "        ret = []\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m piece \u001b[38;5;28;01min\u001b[39;00m regex.findall(_unused_pat, text):\n",
      "            ret.extend(self._core_bpe.encode_single_piece(piece.encode(\u001b[33m\"utf-8\"\u001b[39m)))\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _encode_bytes(self, text: bytes) -> list[int]:\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self._core_bpe._encode_bytes(text)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __getstate__(self) -> object:\n",
      "        \u001b[38;5;28;01mimport\u001b[39;00m tiktoken.registry\n",
      "\n",
      "        \u001b[38;5;66;03m# As an optimisation, pickle registered encodings by reference\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self \u001b[38;5;28;01mis\u001b[39;00m tiktoken.registry.ENCODINGS.get(self.name):\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self.name\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m {\n",
      "            \u001b[33m\"name\"\u001b[39m: self.name,\n",
      "            \u001b[33m\"pat_str\"\u001b[39m: self._pat_str,\n",
      "            \u001b[33m\"mergeable_ranks\"\u001b[39m: self._mergeable_ranks,\n",
      "            \u001b[33m\"special_tokens\"\u001b[39m: self._special_tokens,\n",
      "        }\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __setstate__(self, value: object) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mimport\u001b[39;00m tiktoken.registry\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m isinstance(value, str):\n",
      "            self.__dict__ = tiktoken.registry.get_encoding(value).__dict__\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "        self.__init__(**value)\n",
      "\u001b[31mInit docstring:\u001b[39m\n",
      "Creates an Encoding object.\n",
      "\n",
      "See openai_public.py for examples of how to construct an Encoding object.\n",
      "\n",
      "Args:\n",
      "    name: The name of the encoding. It should be clear from the name of the encoding\n",
      "        what behaviour to expect, in particular, encodings with different special tokens\n",
      "        should have different names.\n",
      "    pat_str: A regex pattern string that is used to split the input text.\n",
      "    mergeable_ranks: A dictionary mapping mergeable token bytes to their ranks. The ranks\n",
      "        must correspond to merge priority.\n",
      "    special_tokens: A dictionary mapping special token strings to their token values.\n",
      "    explicit_n_vocab: The number of tokens in the vocabulary. If provided, it is checked\n",
      "        that the number of mergeable tokens and special tokens is equal to this number."
     ]
    }
   ],
   "source": [
    "# get help\n",
    "tokenizer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "y3wREWipBBH0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100277"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab size\n",
    "tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pIox41g_BBFS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([tokenizer.eot_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aQex4QV7ZC8D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100277\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Invalid token for decoding: 100261'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# but not all tokens are valid, e.g.,\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenizer.n_vocab)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m100261\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LLM_course/Part1_TokensEmbeddings/text2numbers/.venv/lib/python3.12/site-packages/tiktoken/core.py:284\u001b[39m, in \u001b[36mEncoding.decode\u001b[39m\u001b[34m(self, tokens, errors)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: Sequence[\u001b[38;5;28mint\u001b[39m], errors: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    273\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Decodes a list of tokens into a string.\u001b[39;00m\n\u001b[32m    274\u001b[39m \n\u001b[32m    275\u001b[39m \u001b[33;03m    WARNING: the default behaviour of this function is lossy, since decoded bytes are not\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    282\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_core_bpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m, errors=errors)\n",
      "\u001b[31mKeyError\u001b[39m: 'Invalid token for decoding: 100261'"
     ]
    }
   ],
   "source": [
    "# but not all tokens are valid, e.g.,\n",
    "print(tokenizer.n_vocab)\n",
    "tokenizer.decode([100261])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaGtXMdoZDAl"
   },
   "outputs": [],
   "source": [
    "# list of all tokens:\n",
    "# https://github.com/vnglst/gpt4-tokens/blob/main/decode-tokens.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTBxIVVoBBCw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2UiNQED81ig"
   },
   "source": [
    "# Explore some tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aKLYdHUiLalD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 = indow\n",
      "1001 = lement\n",
      "1002 = pect\n",
      "1003 = ash\n",
      "1004 = [i\n",
      "1005 =  use\n",
      "1006 = .F\n",
      "1007 = pec\n",
      "1008 =  ad\n",
      "1009 = ove\n",
      "1010 = ception\n",
      "1011 = ength\n",
      "1012 = include\n",
      "1013 = ader\n",
      "1014 =                            \n",
      "1015 = atus\n",
      "1016 = Th\n",
      "1017 = itle\n",
      "1018 = rit\n",
      "1019 = void\n",
      "1020 = ().\n",
      "1021 = (\n",
      "\n",
      "1022 =  off\n",
      "1023 =  other\n",
      "1024 =  &&\n",
      "1025 = ';\n",
      "\n",
      "1026 = ms\n",
      "1027 =  been\n",
      "1028 =  te\n",
      "1029 = ml\n",
      "1030 = co\n",
      "1031 = nc\n",
      "1032 = 13\n",
      "1033 = ervice\n",
      "1034 =  %\n",
      "1035 = **\n",
      "\n",
      "1036 = ann\n",
      "1037 = ade\n",
      "1038 = \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1039 = lock\n",
      "1040 = const\n",
      "1041 = 100\n",
      "1042 = ponse\n",
      "1043 =  sup\n",
      "1044 = ++\n",
      "1045 = date\n",
      "1046 =  acc\n",
      "1047 =  had\n",
      "1048 =  bu\n",
      "1049 = 200\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000,1050):\n",
    "  print(f'{i} = {tokenizer.decode([i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-0XBTjgLahX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W54bWoRcF6fd"
   },
   "source": [
    "# Tokenization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EQoW6wxE_gBI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5159, 836, 374, 11519, 323, 358, 1093, 26588, 57968, 12556, 76486, 18414, 13]\n",
      "My\n",
      " name\n",
      " is\n",
      " Mike\n",
      " and\n",
      " I\n",
      " like\n",
      " tooth\n",
      "paste\n",
      "-fl\n",
      "avored\n",
      " chocolate\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "text = \"My name is Mike and I like toothpaste-flavored chocolate.\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "\n",
    "for t in tokens:\n",
    "    print(tokenizer.decode([t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SVemggKqXyQL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Mike',\n",
       " 'and',\n",
       " 'I',\n",
       " 'like',\n",
       " 'toothpaste-flavored',\n",
       " 'chocolate.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9wt9WuiI_f-U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"My\" comprises token(s) [5159]\n",
      "\"name\" comprises token(s) [609]\n",
      "\"is\" comprises token(s) [285]\n",
      "\"Mike\" comprises token(s) [35541]\n",
      "\"and\" comprises token(s) [438]\n",
      "\"I\" comprises token(s) [40]\n",
      "\"like\" comprises token(s) [4908]\n",
      "\"toothpaste-flavored\" comprises token(s) [998, 8942, 57968, 12556, 76486]\n",
      "\"chocolate.\" comprises token(s) [331, 14136, 13]\n"
     ]
    }
   ],
   "source": [
    "for word in text.split():\n",
    "  print(f'\"{word}\" comprises token(s) {tokenizer.encode(word)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OwUd-Jry_f7o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token   5159 is \"My\"\n",
      "Token    836 is \" name\"\n",
      "Token    374 is \" is\"\n",
      "Token  11519 is \" Mike\"\n",
      "Token    323 is \" and\"\n",
      "Token    358 is \" I\"\n",
      "Token   1093 is \" like\"\n",
      "Token  26588 is \" tooth\"\n",
      "Token  57968 is \"paste\"\n",
      "Token  12556 is \"-fl\"\n",
      "Token  76486 is \"avored\"\n",
      "Token  18414 is \" chocolate\"\n",
      "Token     13 is \".\"\n"
     ]
    }
   ],
   "source": [
    "for t in tokens:\n",
    "  print(f'Token {t:>6} is \"{tokenizer.decode([t])}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V86VfNz3_f4y"
   },
   "outputs": [],
   "source": [
    "# with special (non-ASCII) characters\n",
    "tokenizer.encode('â')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73ZpWc09F2yT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v28C8R_gF4XK"
   },
   "source": [
    "# How long are the tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HN2EMOYi6ouB"
   },
   "outputs": [],
   "source": [
    "# initialize lengths vector\n",
    "token_lengths = np.zeros(tokenizer.n_vocab)\n",
    "\n",
    "# get the number of characters in each token\n",
    "for idx in range(tokenizer.n_vocab):\n",
    "  try:\n",
    "    token_lengths[idx] = len(tokenizer.decode([idx]))\n",
    "  except:\n",
    "    token_lengths[idx] = np.nan\n",
    "\n",
    "# count unique lengths\n",
    "uniqueLengths,tokenCount = np.unique(token_lengths,return_counts=True)\n",
    "\n",
    "\n",
    "\n",
    "# visualize\n",
    "_,axs = plt.subplots(1,2,figsize=(12,4))\n",
    "axs[0].plot(token_lengths,'k.',markersize=3,alpha=.4)\n",
    "axs[0].set(xlim=[0,tokenizer.n_vocab],xlabel='Token index',ylabel='Token length (characters)',\n",
    "           title='GPT4 token lengths')\n",
    "\n",
    "axs[1].bar(uniqueLengths,tokenCount,color='k',edgecolor='gray')\n",
    "axs[1].set(xlim=[0,max(uniqueLengths)],xlabel='Token length (chars)',ylabel='Token count (log scale)',\n",
    "           title='Distribution of token lengths')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9WjkvGzDa65"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shSBSrGSoHL3"
   },
   "source": [
    "# Many word-tokens start with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Q18PZz3oK1j"
   },
   "outputs": [],
   "source": [
    "# single-token words with vs. without spaces\n",
    "print( tokenizer.encode(' Michael') )\n",
    "print( tokenizer.encode('Michael') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ul4qzL0VoUqP"
   },
   "outputs": [],
   "source": [
    "# multi-token words without a space\n",
    "print( tokenizer.encode(' Peach') )\n",
    "print( tokenizer.encode('Peach') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-YLajcooHJP"
   },
   "outputs": [],
   "source": [
    "peach = tokenizer.encode('Peach')\n",
    "[tokenizer.decode([p]) for p in peach]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41SHGKIsoHGF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsfMaYxen8oU"
   },
   "source": [
    "# The Time Machine book encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-oWizBu_fzO"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n",
    "\n",
    "# split by punctuation\n",
    "words = re.split(r'([,.:;—?_!\"“()\\']|--|\\s)',text)\n",
    "words = [item.strip() for item in words if item.strip()]\n",
    "print(f'There are {len(words)} words.')\n",
    "words[10000:10050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLR0jdSclljl"
   },
   "outputs": [],
   "source": [
    "# tokens of a random word in the text\n",
    "someRandomWord = np.random.choice(words)\n",
    "print(f'\"{someRandomWord}\" has token {tokenizer.encode(someRandomWord)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D39CEFi__fwi"
   },
   "outputs": [],
   "source": [
    "for t in words[:20]:\n",
    "  print(f'\"{t}\" has {len(tokenizer.encode(t))} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkP7Fdcg_ftn"
   },
   "outputs": [],
   "source": [
    "for spelling in ['book','Book','bOok']:\n",
    "  print(f'\"{spelling}\" has tokens {tokenizer.encode(spelling)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_8A_ey6r8qL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVvhTKdDHISD"
   },
   "source": [
    "# But do we need to separate the text into words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntdIA6qMr8lk"
   },
   "outputs": [],
   "source": [
    "# what happens if we just tokenize the raw (unprocessed) text?\n",
    "tmTokens = tokenizer.encode(text)\n",
    "print(f'The text has {len(tmTokens):,} tokens and {len(words):,} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xQRRWpfHQG2"
   },
   "outputs": [],
   "source": [
    "# check out some tokens\n",
    "\n",
    "for t in tmTokens[9990:10020]:\n",
    "  print(f'Token {t:>6}: \"{tokenizer.decode([t])}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WE4HmJ8JBXCB"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tmTokens[9990:10020]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZdw1sP8IVuz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP5pyA9+pTsYfvQ6VmRqRSO",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
