{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBsNRoJ8HpGk"
   },
   "source": [
    "|<h2>Course:</h2>|<h1><a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">A deep understanding of AI language model mechanisms</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Part 1:</h2>|<h1>Tokenizations and embeddings<h1>|\n",
    "|<h2>Section:</h2>|<h1>Words to tokens to numbers<h1>|\n",
    "|<h2>Lecture:</h2>|<h1><b>Parsing text to numbered tokens<b></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<h5><b>Teacher:</b> Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h5>\n",
    "<h5><b>Course URL:</b> <a href=\"https://udemy.com/course/dullms_x/?couponCode=202508\" target=\"_blank\">udemy.com/course/dullms_x/?couponCode=202508</a></h5>\n",
    "<i>Using the code without the course may lead to confusion or errors.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDVo9UpRHuaY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcl6mvSJ0z6H"
   },
   "source": [
    "# Parsing text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RmFWYsyFHuXT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All that we are is the result of what we have thought',\n",
       " 'To be or not to be that is the question',\n",
       " 'Be yourself everyone else is already taken']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the text\n",
    "text = [ 'All that we are is the result of what we have thought',\n",
    "         'To be or not to be that is the question',\n",
    "         'Be yourself everyone else is already taken' ]\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "83q3ZHR6HuSd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_429435/1325930493.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  re.split('\\s',text[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['All',\n",
       " 'that',\n",
       " 'we',\n",
       " 'are',\n",
       " 'is',\n",
       " 'the',\n",
       " 'result',\n",
       " 'of',\n",
       " 'what',\n",
       " 'we',\n",
       " 'have',\n",
       " 'thought']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate into words by splitting by spaces\n",
    "import re\n",
    "re.split('\\s',text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F3xIl0TZHuPx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_429435/1857155964.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  ' '.join( re.split('\\s',text[0]) )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'All that we are is the result of what we have thought'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can recombine into a text\n",
    "' '.join( re.split('\\s',text[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5jMgUhoFJFCo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_429435/2651653478.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  allwords = re.split('\\s',' '.join(text).lower())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'that',\n",
       " 'we',\n",
       " 'are',\n",
       " 'is',\n",
       " 'the',\n",
       " 'result',\n",
       " 'of',\n",
       " 'what',\n",
       " 'we',\n",
       " 'have',\n",
       " 'thought',\n",
       " 'to',\n",
       " 'be',\n",
       " 'or',\n",
       " 'not',\n",
       " 'to',\n",
       " 'be',\n",
       " 'that',\n",
       " 'is',\n",
       " 'the',\n",
       " 'question',\n",
       " 'be',\n",
       " 'yourself',\n",
       " 'everyone',\n",
       " 'else',\n",
       " 'is',\n",
       " 'already',\n",
       " 'taken']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also make lower-case\n",
    "allwords = re.split('\\s',' '.join(text).lower())\n",
    "allwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWjXaGF40-Y2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3vIFNqe0-V7"
   },
   "source": [
    "# Create a vocabulary (lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Xt6qDnGvHuNA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'already',\n",
       " 'are',\n",
       " 'be',\n",
       " 'else',\n",
       " 'everyone',\n",
       " 'have',\n",
       " 'is',\n",
       " 'not',\n",
       " 'of',\n",
       " 'or',\n",
       " 'question',\n",
       " 'result',\n",
       " 'taken',\n",
       " 'that',\n",
       " 'the',\n",
       " 'thought',\n",
       " 'to',\n",
       " 'we',\n",
       " 'what',\n",
       " 'yourself']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the unique words\n",
    "vocab = sorted(set(allwords))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bJIoqHSLHuHL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29 words in the text, and 21 words in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(allwords)} words in the text, and {len(vocab)} words in the vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LNdk--11DH9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQOK9BE31DDd"
   },
   "source": [
    "# Create an encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RQk8dGyBHuEc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': 0,\n",
       " 'already': 1,\n",
       " 'are': 2,\n",
       " 'be': 3,\n",
       " 'else': 4,\n",
       " 'everyone': 5,\n",
       " 'have': 6,\n",
       " 'is': 7,\n",
       " 'not': 8,\n",
       " 'of': 9,\n",
       " 'or': 10,\n",
       " 'question': 11,\n",
       " 'result': 12,\n",
       " 'taken': 13,\n",
       " 'that': 14,\n",
       " 'the': 15,\n",
       " 'thought': 16,\n",
       " 'to': 17,\n",
       " 'we': 18,\n",
       " 'what': 19,\n",
       " 'yourself': 20}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the encoder is a python dictionary type\n",
    "word2idx = {}\n",
    "for i,word in enumerate(vocab):\n",
    "  word2idx[word] = i\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PqKALTBaHuBa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'all',\n",
       " 1: 'already',\n",
       " 2: 'are',\n",
       " 3: 'be',\n",
       " 4: 'else',\n",
       " 5: 'everyone',\n",
       " 6: 'have',\n",
       " 7: 'is',\n",
       " 8: 'not',\n",
       " 9: 'of',\n",
       " 10: 'or',\n",
       " 11: 'question',\n",
       " 12: 'result',\n",
       " 13: 'taken',\n",
       " 14: 'that',\n",
       " 15: 'the',\n",
       " 16: 'thought',\n",
       " 17: 'to',\n",
       " 18: 'we',\n",
       " 19: 'what',\n",
       " 20: 'yourself'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and a decoder\n",
    "idx2word = {}\n",
    "for i,word in enumerate(vocab):\n",
    "  idx2word[i] = word\n",
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NTR0BMxdHt-l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"to\" has index 17\n",
      "The index \"7\" maps to the word \"is\"\n"
     ]
    }
   ],
   "source": [
    "print(f'The word \"to\" has index {word2idx[\"to\"]}')\n",
    "print(f'The index \"7\" maps to the word \"{idx2word[7]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1VfwvVt1NUz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GULhYnOP1NSA"
   },
   "source": [
    "# Make fake quotes, just for fun :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QGCwhK-GHt8B"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['already', 'question', 'yourself', 'everyone', 'everyone', 'are']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select random words from the dictionary\n",
    "import numpy as np\n",
    "randidx = np.random.randint(0,len(vocab),size=6)\n",
    "\n",
    "# words of wisdom as a list of tokens\n",
    "[ idx2word[i] for i in randidx ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "d0QzTDjSUU-E"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'already question yourself everyone everyone are'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# does it sound more wise as text??\n",
    "' '.join([ idx2word[i] for i in randidx ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0StRY37I1dwU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_X2k966q1dpW"
   },
   "source": [
    "# A peak at tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "rLBJ8ijaL5um"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 14,\n",
       " 18,\n",
       " 2,\n",
       " 7,\n",
       " 15,\n",
       " 12,\n",
       " 9,\n",
       " 19,\n",
       " 18,\n",
       " 6,\n",
       " 16,\n",
       " 17,\n",
       " 3,\n",
       " 10,\n",
       " 8,\n",
       " 17,\n",
       " 3,\n",
       " 14,\n",
       " 7,\n",
       " 15,\n",
       " 11,\n",
       " 3,\n",
       " 20,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 13]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translate the text into numbers\n",
    "text_as_int = [ word2idx[word] for word in allwords ]\n",
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "D9VtmaEb15kX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token  0: all\n",
      "Token 14: that\n",
      "Token 18: we\n",
      "Token  2: are\n",
      "Token  7: is\n",
      "Token 15: the\n",
      "Token 12: result\n",
      "Token  9: of\n",
      "Token 19: what\n",
      "Token 18: we\n",
      "Token  6: have\n",
      "Token 16: thought\n",
      "Token 17: to\n",
      "Token  3: be\n",
      "Token 10: or\n",
      "Token  8: not\n",
      "Token 17: to\n",
      "Token  3: be\n",
      "Token 14: that\n",
      "Token  7: is\n",
      "Token 15: the\n",
      "Token 11: question\n",
      "Token  3: be\n",
      "Token 20: yourself\n",
      "Token  5: everyone\n",
      "Token  4: else\n",
      "Token  7: is\n",
      "Token  1: already\n",
      "Token 13: taken\n"
     ]
    }
   ],
   "source": [
    "# and numbers back into text\n",
    "for tokeni in text_as_int:\n",
    "  print(f'Token {tokeni:2}: {idx2word[tokeni]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beWoY-qYOyLn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPmDafzaGUTDQyd44z4n9E9",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
